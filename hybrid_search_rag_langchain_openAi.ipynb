{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b71226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies into *this notebook kernel* (prefer %pip over !python -m pip)\n",
    "%pip install -q pypdf langchain langchain_community langchain_openai langchain_chroma rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147d36f",
   "metadata": {},
   "source": [
    "#### Initialize OpeAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e56028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\", temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a34a2",
   "metadata": {},
   "source": [
    "#### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd52bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89966f9a",
   "metadata": {},
   "source": [
    "#### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f3a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Research.pdf\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9f893",
   "metadata": {},
   "source": [
    "#### Split Document into chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0740f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce15425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab6f6e",
   "metadata": {},
   "source": [
    "#### Create Semantic Search Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e545bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(chunks,embedding_model)\n",
    "\n",
    "vector_store_retriver= vector_store.as_retriever( search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549c980d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002C313432570>, search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_retriver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90e41b",
   "metadata": {},
   "source": [
    "#### Create Keyword Search Retiever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d185d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "\n",
    "keyword_retriever.k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d7735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002C35A44AEA0>, k=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cb434",
   "metadata": {},
   "source": [
    "#### Create Hybrid search Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff1c3f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:404: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HybridRetriever(vector_retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000002C313432570>, search_kwargs={'k': 2}), keyword_retriever=BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002C35A44AEA0>, k=2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hybrid search: combine semantic + keyword retrievers (no Embedchain client required)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    # BaseRetriever is available in most LangChain installs via langchain-core\n",
    "    from langchain_core.retrievers import BaseRetriever\n",
    "except Exception:\n",
    "    BaseRetriever = object  # fallback (still lets you call `hybrid_search(...)` directly)\n",
    "\n",
    "def _retrieve(retriever, query: str):\n",
    "    \"\"\"Compatibility helper: works across different LangChain retriever interfaces.\"\"\"\n",
    "    if hasattr(retriever, \"invoke\"):\n",
    "        return retriever.invoke(query)\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def _doc_key(doc):\n",
    "    # stable key for de-duplication across retrievers\n",
    "    meta_items = tuple(sorted((doc.metadata or {}).items()))\n",
    "    return (doc.page_content, meta_items)\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    vector_retriever,\n",
    "    keyword_retriever,\n",
    "    k: int = 4,\n",
    "    weights=(0.5, 0.5),\n",
    "):\n",
    "    sem_docs = _retrieve(vector_retriever, query)\n",
    "    key_docs = _retrieve(keyword_retriever, query)\n",
    "\n",
    "    scores = defaultdict(float)\n",
    "    docs_by_key = {}\n",
    "\n",
    "    for rank, doc in enumerate(sem_docs):\n",
    "        key = _doc_key(doc)\n",
    "        docs_by_key[key] = doc\n",
    "        scores[key] += weights[0] * (1.0 / (rank + 1))\n",
    "\n",
    "    for rank, doc in enumerate(key_docs):\n",
    "        key = _doc_key(doc)\n",
    "        docs_by_key[key] = doc\n",
    "        scores[key] += weights[1] * (1.0 / (rank + 1))\n",
    "\n",
    "    ranked_keys = sorted(scores.keys(), key=lambda key: scores[key], reverse=True)\n",
    "    return [docs_by_key[key] for key in ranked_keys[:k]]\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    vector_retriever: any\n",
    "    keyword_retriever: any\n",
    "    k: int = 4\n",
    "    weights: tuple = (0.5, 0.5)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager=None):\n",
    "        return hybrid_search(\n",
    "            query,\n",
    "            vector_retriever=self.vector_retriever,\n",
    "            keyword_retriever=self.keyword_retriever,\n",
    "            k=self.k,\n",
    "            weights=self.weights,\n",
    "        )\n",
    "\n",
    "ensemble_retriever = HybridRetriever(\n",
    "    vector_retriever=vector_store_retriver,\n",
    "    keyword_retriever=keyword_retriever,\n",
    "    k=4,\n",
    "    weights=(0.5, 0.5),\n",
    ")\n",
    "\n",
    "ensemble_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690b394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
