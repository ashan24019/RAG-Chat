{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667dfcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-5-nano-2025-08-07\", temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6c518",
   "metadata": {},
   "source": [
    "#### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f71dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807bb1e",
   "metadata": {},
   "source": [
    "#### Load PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e002ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"Research.pdf\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e910aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bacdd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Skia/PDF m140', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36', 'creationdate': '2025-09-23T15:50:49+00:00', 'source': 'Research.pdf', 'file_path': 'Research.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Neural Network Pruning Research Proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T15:50:49+00:00', 'trapped': '', 'modDate': \"D:20250923155049+00'00'\", 'creationDate': \"D:20250923155049+00'00'\", 'page': 0}, page_content='Neural Network Pruning Research Proposal\\n1. Introduction & Context\\nThe Challenge of Model Efficiency\\nModern deep neural networks have achieved remarkable performance across various domains, from\\ncomputer vision to natural language processing. However, their success comes at a significant\\ncomputational cost. State-of-the-art models like GPT-4, BERT-Large, or ResNet-152 contain millions to\\nbillions of parameters, requiring substantial memory, computational power, and energy consumption.\\nThis presents a critical bottleneck for deploying AI models in resource-constrained environments such\\nas mobile devices, edge computing systems, and Internet of Things (IoT) applications.\\nThe Promise of Neural Network Pruning\\nNeural network pruning emerges as a compelling solution to this efficiency challenge. The core\\ninsight is that many neural networks are over-parameterized, containing redundant connections and\\nneurons that contribute minimally to the model\\'s performance. By systematically removing these less\\nimportant parameters, pruning techniques can significantly reduce model size and computational\\nrequirements while maintaining acceptable accuracy levels.\\nCurrent Landscape and Limitations\\nExisting pruning approaches can be broadly categorized into structured and unstructured pruning\\nmethods. While magnitude-based pruning (removing weights with smallest absolute values) and\\nlottery ticket hypothesis have shown promise, most current techniques rely on static criteria that don\\'t\\nadapt to the specific characteristics of different datasets or training dynamics. This one-size-fits-all\\napproach often leads to suboptimal trade-offs between model compression and performance\\nretention.\\nResearch Gap and Opportunity\\nThe field lacks sophisticated adaptive pruning strategies that can intelligently adjust their behavior\\nbased on real-time training dynamics, dataset characteristics, and model architecture specifics. This\\npresents an opportunity to develop more nuanced approaches that can achieve better compression-\\nperformance trade-offs through dynamic adaptation.\\n2. Tentative Research Topic\\n\"Adaptive Neural Network Pruning: Dynamic Strategies for Optimal Model Compression Based on\\nTraining Dynamics and Data Characteristics\"')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d5ecf",
   "metadata": {},
   "source": [
    "#### Split Document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c29b7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ab55cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0ee279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Skia/PDF m140', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36', 'creationdate': '2025-09-23T15:50:49+00:00', 'source': 'Research.pdf', 'file_path': 'Research.pdf', 'total_pages': 3, 'format': 'PDF 1.4', 'title': 'Neural Network Pruning Research Proposal', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-23T15:50:49+00:00', 'trapped': '', 'modDate': \"D:20250923155049+00'00'\", 'creationDate': \"D:20250923155049+00'00'\", 'page': 0}, page_content='Neural Network Pruning Research Proposal\\n1. Introduction & Context\\nThe Challenge of Model Efficiency\\nModern deep neural networks have achieved remarkable performance across various domains, from\\ncomputer vision to natural language processing. However, their success comes at a significant\\ncomputational cost. State-of-the-art models like GPT-4, BERT-Large, or ResNet-152 contain millions to')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526d229",
   "metadata": {},
   "source": [
    "#### Create Vector Store and Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5349217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d151b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vectorstore\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadb189",
   "metadata": {},
   "source": [
    "#### Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c1885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful AI assistant that helps people find information \"\n",
    "    \"from a set of documents. Use the following pieces of context to answer \"\n",
    "    \"the question at the end. If you don't know the answer, just say that you \"\n",
    "    \"don't know, don't try to make up an answer.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# NOTE: RetrievalQA passes the user's question under the variable name 'question'\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd876c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are a helpful AI assistant that helps people find information from a set of documents. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c24405",
   "metadata": {},
   "source": [
    "#### Create Retrieval-Augmented Generation (RAG) chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defe48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# RAG chain (compatible with your installed LangChain)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc977b",
   "metadata": {},
   "source": [
    "#### Invoke RAG Chain with example questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1609e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response keys: ['query', 'result']\n",
      "answer:\n",
      " I don’t know the answer from the provided documents—the materials you shared are about neural network pruning and don’t contain information about rivers. If you’d like, I can summarize the pruning proposal or help with related questions.\n"
     ]
    }
   ],
   "source": [
    "# RetrievalQA expects the key 'query'\n",
    "response = rag_chain.invoke({\"query\": \"What is longest river in the world?\"})\n",
    "\n",
    "# Show what came back\n",
    "print(\"response keys:\", list(response.keys()))\n",
    "print(\"answer:\\n\", response.get(\"result\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2089aef",
   "metadata": {},
   "source": [
    "#### Add chat history (For keeping continues chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "600c658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Define the contextualize system prompt\n",
    "contextualize_system_prompt = (\n",
    "    \"Using the conversation history and the latest user input, \"\n",
    "    \"reformulate the input into a standalone question if needed; \"\n",
    "    \"otherwise return it as-is.\"\n",
    ")\n",
    "\n",
    "# IMPORTANT: create_history_aware_retriever expects the variable name 'input'\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the history-aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chat_model,\n",
    "    retriever,\n",
    "    contextualize_prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afe3eb",
   "metadata": {},
   "source": [
    "#### Create history aware RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7922a288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000270712E8C20>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are a helpful AI assistant that helps people find information from a set of documents. Use the following pieces of context to answer the user input at the end. If you don't know the answer, just say that you don't know.\\n\\n{context}\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Prompt used for answering, with chat history included\n",
    "history_system_prompt = (\n",
    "    \"You are a helpful AI assistant that helps people find information \"\n",
    "    \"from a set of documents. Use the following pieces of context to answer \"\n",
    "    \"the user input at the end. If you don't know the answer, just say that you \"\n",
    "    \"don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "history_qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", history_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_qa_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d5aba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Build a history-aware RAG runnable.\n",
    "# Note: RetrievalQA.from_chain_type can't accept history_aware_retriever because it's a Runnable\n",
    "# (it needs both 'input' and 'chat_history'), not a BaseRetriever.\n",
    "\n",
    "def _history_aware_rag(inputs: dict) -> dict:\n",
    "    user_input = inputs[\"input\"]\n",
    "    chat_history = inputs.get(\"chat_history\", [])\n",
    "\n",
    "    docs = history_aware_retriever.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs[:4])\n",
    "\n",
    "    messages = history_qa_prompt.format_messages(\n",
    "        context=context,\n",
    "        input=user_input,\n",
    "        chat_history=chat_history,\n",
    "    )\n",
    "    answer_msg = chat_model.invoke(messages)\n",
    "    answer = getattr(answer_msg, \"content\", str(answer_msg))\n",
    "\n",
    "    return {\"answer\": answer, \"source_documents\": docs}\n",
    "\n",
    "history_aware_rag_chain = RunnableLambda(_history_aware_rag)\n",
    "\n",
    "# ---- Session history store ----\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    history_aware_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246464ea",
   "metadata": {},
   "source": [
    "#### \n",
    "Manage Chat Session History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "103831f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1:\n",
      " - The document outlines a research program on adaptive pruning of neural networks that dynamically adjusts pruning decisions during training.\n",
      "- It proposes a Comprehensive Evaluation Framework to systematically compare pruning techniques, using baselines such as magnitude-based pruning, structured pruning, and lottery ticket methods; and evaluating with metrics like model size reduction, inference speed, accuracy retention, and training efficiency.\n",
      "- It includes a Software Implementation component: an open-source tool/framework for adaptive pruning.\n",
      "- It presents Empirical Insights on how different adaptation strategies affect model performance and generalization.\n",
      "- Expected Contributions: a novel adaptive pruning algorithm and a software engineering implementation to support it.\n",
      "- Research Scope and Boundaries: focus on developing adaptive pruning methods with an emphasis on software implementation; target computer vision architectures (ResNet, EfficientNet) and potentially smaller language models; evaluation domains include image classification and potentially text classification.\n",
      "- Sub-Research Questions:\n",
      "  - Adaptive Criteria Development: which training dynamics indicators (loss landscapes, gradient magnitudes, activation patterns) best guide pruning decisions?\n",
      "  - Multi-Stage Pruning Optimization: how to integrate adaptive pruning across different training phases (early training, fine-tuning, post-training) for maximum effectiveness.\n",
      "\n",
      "--- Follow-up (same session) ---\n",
      "\n",
      "Answer 2:\n",
      " - Novel Adaptive Algorithm: A pruning method that dynamically adjusts pruning decisions during training based on training dynamics.\n",
      "- Comprehensive Evaluation Framework: A systematic methodology to compare pruning techniques, using baselines such as magnitude-based pruning, structured pruning, and lottery ticket methods, and evaluating with metrics like model size reduction, inference speed, accuracy retention, and training efficiency.\n",
      "- Software Implementation: An open-source tool/framework for adaptive pruning.\n",
      "- Empirical Insights: Analysis of how different adaptation strategies affect model performance and generalization.\n"
     ]
    }
   ],
   "source": [
    "session_id = \"101\"\n",
    "\n",
    "resp1 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Give me a summary of the document.\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(\"Answer 1:\\n\", resp1[\"answer\"])\n",
    "\n",
    "print(\"\\n--- Follow-up (same session) ---\\n\")\n",
    "resp2 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are the key contributions?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "print(\"Answer 2:\\n\", resp2[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
